{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-term fitness trajectories from the LTEE\n",
    "\n",
    "Data available in the Supplemental Material of Good et al. Nature 2017. \n",
    "Download possible from Ben Good's github repository [here](https://github.com/benjaminhgood/LTEE-metagenomic/blob/master/additional_data/Concatenated.LTEE.data.all.csv)\n",
    "\n",
    "We follow the procedures from Wiser et al. 2013 [here](https://doi.org/10.1126/science.1243357). From the Supplemental Material, we are given the following information. \n",
    "\n",
    "\n",
    "- Summarizing statistical procedures to fit the two models\n",
    "- Models were fit to fitness trajectories using the ‘nls’ package in r. \n",
    "- Model fits were compared using the BIC information criterion scores. These were then converted into an odds ratio. \n",
    "    - Table S1 shows the BIC scores and odds ratios for fits to subsets of the data: a) all 12 populations and all time points, b) excluding 3 populations with incomplete trajectories and c) excluding 6 populations that evolved hypermutability\n",
    "    - Table S2 summarizes BIC scores for fits to individual populations. This also indicates if the population was truncated or a hypermutator \n",
    "    - Table S4 lists the estimated parameters for the power law fit\n",
    "\n",
    "On the bigger picture, there is also the talk from 2013 by Wiser on [Youtube](https://www.youtube.com/watch?v=CmyBn5Cezy4) with 127 views as of September 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "## create export directory if necessary\n",
    "## foldernames for output plots/lists produced in this notebook\n",
    "import os\n",
    "FIG_DIR = f'./figures/LTEE_fit/'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "print(\"All  plots will be stored in: \\n\" + FIG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Concatenated.LTEE.data.all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### execute script to load modules here\n",
    "exec(open('setup_aesthetics.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explanation of column headers and labels\n",
    "\n",
    "    607 is the wild-type strain 'REL607'\n",
    "    D.0 is the dilution factor applied to count colonies in initial inoculum. \n",
    "    D.1 is the dilution factor applied to count colonies in the saturated population. We expect D.1 = 100*D.0. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### split into two subsets, according to the Ara-marker of evolving population\n",
    "is_Ara_positive = np.array(['+' in v for v in df['Population'].values])\n",
    "\n",
    "df_pos = df[is_Ara_positive]\n",
    "df_neg = df[~is_Ara_positive] ### only use Ara negative lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## treat Ara-posative population\n",
    "assert all(df_pos['Red.Pop'] == '606') # wild-type is always the red population\n",
    "\n",
    "### re-construct population sizes\n",
    "df_pos['Nwt.0'] = df_pos['Red.0']*df_pos['D.0']\n",
    "df_pos['Nmut.0'] = df_pos['White.0']*df_pos['D.0']\n",
    "df_pos['Nwt.1'] = df_pos['Red.1']*df_pos['D.1']\n",
    "df_pos['Nmut.1'] = df_pos['White.1']*df_pos['D.1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## treat Ara-negative population\n",
    "assert all(df_neg['White.Pop'] == '607') # wild-type is always the white population\n",
    "\n",
    "### re-construct population sizes\n",
    "df_neg['Nwt.0'] = df_neg['White.0']*df_neg['D.0']\n",
    "df_neg['Nmut.0'] = df_neg['Red.0']*df_neg['D.0']\n",
    "df_neg['Nwt.1'] = df_neg['White.1']*df_neg['D.1']\n",
    "df_neg['Nmut.1'] = df_neg['Red.1']*df_neg['D.1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join\n",
    "\n",
    "df = df_pos.append(df_neg)\n",
    "\n",
    "## reconstruct frequencies\n",
    "df['xmut.0'] = df['Nmut.0']/(df['Nwt.0'] + df['Nmut.0'])\n",
    "df['xmut.1'] = df['Nmut.1']/(df['Nwt.1']  + df['Nmut.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reconstruct fitness statistics\n",
    "df['s'] = np.log(df['Nmut.1']/df['Nwt.1']) - np.log(df['Nmut.0']/df['Nwt.0'])\n",
    "df['W'] = np.divide( np.log(df['Nmut.1']/df['Nmut.0']),\\\n",
    "                            np.log(df['Nwt.1']/df['Nwt.0']))\n",
    "df['delta_log'] = np.log(df['xmut.1']) - np.log(df['xmut.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check that  my number is consistent with existing value for 'Fitness' in the dataset\n",
    "np.allclose(df['W'], df['Fitness'],equal_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## manual check\n",
    "df['gap'] = df['W'] - df['Fitness']\n",
    "print(df['gap'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_s = 'tab:grey'\n",
    "color_W = 'firebrick'\n",
    "color_deltalog = 'navy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_hyper = 'tab:red'\n",
    "def hyperbolic(t, a, b):\n",
    "    ## compare first Equation in paper\n",
    "    return 1 + np.divide(a*t,t+b)\n",
    "\n",
    "color_power = 'tab:blue'\n",
    "def powerlaw(t, a, b):\n",
    "    ## compare second Equation in paper\n",
    "    return np.power(b*t + 1,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we drop some superfluous columns\n",
    "columns_auxiliary = [ 'Red.0', 'White.0', 'Red.1', 'White.1', 'D.0', 'D.1', 'gap', 'White.Pop', 'Red.Pop', 'Fitness', 'Nwt.0', 'Nmut.0', 'Nwt.1', 'Nmut.1', 'Nwt.0']\n",
    "\n",
    "df = df.drop(columns_auxiliary, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shift data points for alternative statistics, which are based at zero\n",
    "df['s+1'] = df['s'] +1\n",
    "df['delta_log+1'] = df['delta_log'] +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reproduce fits to individual subpopulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_populations = list(set(df['Population'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_populations.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up Dataframe to store results\n",
    "df_results = pd.DataFrame()\n",
    "for pop in list_populations:\n",
    "    df_tmp= pd.DataFrame(data = {'pop':3*[pop], 'target':['W', 's+1', 'delta_log+1']})\n",
    "    df_results = df_results.append(df_tmp)\n",
    "    \n",
    "df_results = df_results.set_index(['pop','target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2) + tuple([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "for target in ['W', 's+1', 'delta_log+1']: \n",
    "    fig, axes = plt.subplots(4,3, figsize = (4*FIGHEIGHT_TRIPLET,3*FIGWIDTH_TRIPLET) , sharex=True, sharey=True)\n",
    "                         \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for pop,ax in zip(list_populations,axes):\n",
    "\n",
    "        df_subset = df[df['Population'] == pop]\n",
    "        ### remove na values\n",
    "        df_subset= df_subset[~df_subset[target].isna()]\n",
    "        df_results.at[(pop,target), 'n_datapoints'] = df_subset.shape[0] # store nubmer of datapoints\n",
    "        df_results.at[(pop, target), 'Generation_max'] = df_subset['Generation'].max() # store final timepoint\n",
    "\n",
    "        t = df_subset['Generation']\n",
    "        y = df_subset[target]\n",
    "        ## plot raw data\n",
    "        sns.scatterplot(x=t,y=y, ax=ax, color = 'grey')\n",
    "        sns.lineplot(x=t,y=y, ax =ax, color = 'grey')\n",
    "        \n",
    "        ## fit hyperbolic model\n",
    "        popt_hyperbolic,_ = curve_fit(f=hyperbolic, xdata=t, ydata=y)\n",
    "        ## store\n",
    "        df_results.at[(pop,target), 'hyper_a'] = popt_hyperbolic[0]\n",
    "        df_results.at[(pop,target), 'hyper_b'] = popt_hyperbolic[1]\n",
    "        ## compute trajectory\n",
    "        y_hat = hyperbolic(t, *popt_hyperbolic)\n",
    "        ## store sum of residuals squared\n",
    "        rss = np.power(y_hat - y,2).sum()\n",
    "        rsquared = 1 - (rss/np.power(y - y.mean(),2).sum())\n",
    "        df_results.at[(pop, target), 'hyper_rss']  = rss\n",
    "        df_results.at[(pop, target), 'hyper_rsquared']  = rsquared\n",
    "        ## plot trajectory\n",
    "        param_label = 'hyperbolic: a=%.2f, b=%.0f' % tuple(popt_hyperbolic)\n",
    "        sns.lineplot(x=t, y =y_hat, color = color_hyper,ax=ax,\n",
    "                     label = param_label + rf\", $R^2={rsquared:.2f}$\")\n",
    "\n",
    "        \n",
    "        # fit powerlaw model\n",
    "        popt_powerlaw,_ = curve_fit(f=powerlaw, xdata=t, ydata=y)\n",
    "        ## store\n",
    "        df_results.at[(pop,target), 'powerlaw_a'] = popt_powerlaw[0]\n",
    "        df_results.at[(pop,target), 'powerlaw_b'] = popt_powerlaw[1]\n",
    "        ## compute trajectory\n",
    "        y_hat = powerlaw(t, *popt_powerlaw)\n",
    "        ## store sum of residuals squared\n",
    "        rss = np.power(y_hat - y,2).sum()\n",
    "        rsquared = 1 - (rss/np.power(y - y.mean(),2).sum())\n",
    "        df_results.at[(pop, target), 'powerlaw_rss']  = rss\n",
    "        df_results.at[(pop, target), 'powerlaw_rsquared']  = rsquared\n",
    "        ## plot trajectory\n",
    "        param_label = 'powerlaw: a=%5.4f, b=%5.5f' % tuple(popt_powerlaw)\n",
    "        sns.lineplot(x=t, y =y_hat, color = color_power,ax=ax,\\\n",
    "                     label = param_label + rf\", $R^2={rsquared:.2f}$\")\n",
    "    \n",
    "    \n",
    "        ax.legend()\n",
    "        ax.set_title(pop, loc = 'right')\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig.savefig(FIG_DIR + f\"fits_to_individual_populations_using_{target}.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)\n",
    "    \n",
    "  ## remove cluttering output\n",
    "    if target != 'W':\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate model comparison stats\n",
    "\n",
    "#### Calculate BIC of the fit manually \n",
    "\n",
    "The BIC according to [Wikipedia](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Gaussian_special_case) for the special case of a Gaussian error distribution can be calculate as \n",
    "\n",
    "$$\n",
    "\\mathrm{BIC} = n \\cdot \\log (\\mathrm{RSS}) + k \\cdot \\log n  - n \\cdot \\log n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $k$ is the number of parameters estimated by the model,\n",
    "- $\\log$ is the natural logarithm\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS} = \\sum_{i=1}^n (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "is the residual sum of squares. Note that we can drop the last term $n*\\log n$, since it is identical for all model fits in our comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute likelihood ratio\n",
    "\n",
    "The maximum of the likelihood function is given by\n",
    "\n",
    "$$\n",
    "\\hat{l} = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}^2) - \\frac{1}{2\\hat{\\sigma}^2}\\cdot \\mathrm{RSS}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\sigma}$ is the reduced $\\chi^2$-statistic (estimate of the error variance?).\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_least_squares)\n",
    "we know that for a Gaussian distribution of errors that\n",
    "\n",
    "\n",
    "$$ \n",
    "\\hat{\\sigma}^2 = \\mathrm{RSS}/n,\n",
    "$$\n",
    "which we insert in the likelihood function to get\n",
    "\n",
    "$$\n",
    "\\hat{l} = -\\frac{n}{2} ( \\log(2\\pi) + 1) - \\frac{n}{2}\\log(\\mathrm{RSS}/n) \n",
    "$$\n",
    "\n",
    "The paper by Wiser et al. in Table S1 refers to an 'odds ratio'. It is not clear how this odds ratio is defined from the BIC. Here we assume that they mean a likelihood ratio. The likelihood ratio between two model fits with log-likelihood $\\hat{l}_A$ and $\\hat{l}_B$ is defined as\n",
    "\n",
    "$$\n",
    "\\text{likelihood ratio}\\qquad p = \\frac{\\exp(\\hat{l}_A)}{\\exp(\\hat{l}_B)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Note that we are dividing the likelihoods on a linear scale $L=\\exp(l)$ because these actually correspond to the probability density.\n",
    "\n",
    "A large value of $p$ is supposed to indicate superiority of model $A$ over model $B$. Naively, this ratio can be interpreted as \n",
    "\n",
    "> the probability that model A is true in a world where there is only model A and B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AIC of the fit manually \n",
    "\n",
    "The AIC according to [Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_least_squares) for the special case of a Gaussian error distribution can be calculate as \n",
    "\n",
    "$$\n",
    "\\mathrm{AIC} = 2k + n \\cdot \\log \\mathrm{RSS}  + C\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- $k$ is the number of parameters estimated by the model,\n",
    "- $C$ is a constant that only depends on the number and value of datapoints,\n",
    "- $\\log$ is the natural logarithm\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathrm{RSS} = \\sum_{i=1}^n (\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "is the residual sum of squares. Note that we can drop the last term $C$, since it is identical for all model fits in our comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "\n",
    "k = 3 # include one extra parameter for variance of error distribution\n",
    "\n",
    "\n",
    "for pop in list_populations:\n",
    "    for target  in ['W', 's+1', 'delta_log+1']:\n",
    "\n",
    "        ## read number of datapoints\n",
    "        n = df_results.at[(pop, target),'n_datapoints']\n",
    "\n",
    "        ## read hyperbolic model results\n",
    "        rss = df_results.at[(pop, target),'hyper_rss']\n",
    "        ## compute information criteria\n",
    "        aic_hyper = 2*k + n*np.log(rss)\n",
    "        bic_hyper = n*np.log(rss) + k*np.log(n)\n",
    "        likelihood_hyper = -n/2*(np.log(2*np.pi) + 1) -n/2*np.log(rss/n)\n",
    "        ## store\n",
    "        df_results.at[(pop, target),'hyper_aic'] = aic_hyper\n",
    "        df_results.at[(pop, target),'hyper_bic'] = bic_hyper\n",
    "        df_results.at[(pop, target),'hyper_likelihood'] = likelihood_hyper\n",
    "\n",
    "        ## read powerlaw model results\n",
    "        rss = df_results.at[(pop, target),'powerlaw_rss']\n",
    "        ## compute information criteria\n",
    "        aic_powerlaw = 2*k + n*np.log(rss)\n",
    "        bic_powerlaw = n*np.log(rss) + k*np.log(n)\n",
    "        likelihood_powerlaw = -n/2*(np.log(2*np.pi) + 1) -n/2*np.log(rss/n)\n",
    "        ## store\n",
    "        df_results.at[(pop, target),'powerlaw_aic'] = aic_powerlaw\n",
    "        df_results.at[(pop, target),'powerlaw_bic'] = bic_powerlaw\n",
    "        df_results.at[(pop, target),'powerlaw_likelihood'] = likelihood_powerlaw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute model comparison stats based on BIC\n",
    "\n",
    "df_results['delta_bic'] = df_results['hyper_bic'] - df_results['powerlaw_bic']\n",
    "df_results['likelihood_ratio'] = np.exp(df_results['powerlaw_likelihood'] - df_results['hyper_likelihood'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coompute model comparison stats based on AIC\n",
    "df_results['delta_aic'] = df_results['hyper_aic'] - df_results['powerlaw_aic']\n",
    "\n",
    "for pop in list_populations:\n",
    "    for target  in ['W', 's+1', 'delta_log+1']:\n",
    "        aic_hyper = df_results.at[(pop, target), 'hyper_aic']\n",
    "        aic_powerlaw = df_results.at[(pop, target), 'powerlaw_aic']\n",
    "\n",
    "        ## identify model with smaller aic\n",
    "        aic_min = np.min([aic_hyper,aic_powerlaw])\n",
    "        aic_max = np.max([aic_hyper,aic_powerlaw])\n",
    "\n",
    "        ## compute probability\n",
    "        ### see https://en.wikipedia.org/wiki/Akaike_information_criterion#How_to_use_AIC_in_practice\n",
    "        prob_max_model_is_better = np.exp((aic_min - aic_max)/2)\n",
    "\n",
    "\n",
    "        df_results.at[(pop, target), 'akaike_pvalue'] = prob_max_model_is_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_print = ['hyper_bic', 'powerlaw_bic', 'delta_bic', 'likelihood_ratio']\n",
    "rows_to_print =[v for v in df_results.index if v[1] == 'W']\n",
    "df_to_print = df_results.loc[rows_to_print,col_to_print].copy(deep=True)\n",
    "\n",
    "df_to_print = df_to_print.reset_index(level = 'target', drop=True)\n",
    "order = [f'Ara - {v}' for v in range(1,7)] + [f'Ara + {v}' for v in range(1,7)] \n",
    "df_to_print.loc[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### store as excel file\n",
    "## save\n",
    "df_to_print.to_csv(FIG_DIR + 'reproduce_TableS2_stats_for_single_population_fits.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
