{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Plot results from Warringer fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### execute script to load modules here\n",
    "exec(open('setup_aesthetics.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "## cell flagged with tag parameters\n",
    "### parameters for merging plateaus\n",
    "\n",
    "DATASET = 'warringer2003'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update dependent parameters according to input\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "FIG_DIR = f'./figures/{DATASET}/'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "print(\"All  plots will be stored in: \\n\" + FIG_DIR)\n",
    "\n",
    "\n",
    "OUTPUT_DIR = f'./output/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"All  newly created datafiles will be stored in: \\n\" + OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = f'./data/{DATASET}/quantified/'\n",
    "\n",
    "\n",
    "## read and write on results from piecewise fit \n",
    "PCWS_DATA_PLATEAUS = DATA_DIR + 'df_plateaus.csv'\n",
    "PCWS_DATA_TRANSITIONS = DATA_DIR + 'df_transitions.csv'\n",
    "PCWS_DATA_SHOULDERS = DATA_DIR + 'df_shoulders.csv'\n",
    "PCWS_DATA_STATS = DATA_DIR + 'stats_by_curve.csv'\n",
    "\n",
    "# create a new dataframe for the transition phases\n",
    "PCWS_DATA_TRANSITION_PHASES = DATA_DIR + 'df_transitions_by_phase.csv'\n",
    "\n",
    "DATA_DIR = f'./data/{DATASET}/piecewise_fit/'\n",
    "# read some timecourses from the piecewise fit\n",
    "PCWS_DATA_DLOGOD_TIMEPOINTS = DATA_DIR + 'dlogod_timepoints.csv'\n",
    "PCWS_DATA_DLOGOD_VALUES = DATA_DIR + 'dlogod_values.csv'\n",
    "PCWS_DATA_LOGOD_TIMEPOINTS = DATA_DIR + 'logod_timepoints.csv'\n",
    "PCWS_DATA_LOGOD_VALUES = DATA_DIR + 'logod_values.csv'\n",
    "\n",
    "\n",
    "\n",
    "SETUP_SCRIPT = f'setup_plateau_finder_{DATASET}.py'\n",
    "\n",
    "assert path.isfile(SETUP_SCRIPT), f\"Setup script: {SETUP_SCRIPT} does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2colors = {'campos2018':'navy', 'chevereau2015':'firebrick', 'warringer2003':'darkorange' }\n",
    "DATASET_COLOR = dataset2colors[DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(SETUP_SCRIPT).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the same interpolate function as for the plateau finding\n",
    "def interpolate(x, xp, fp):\n",
    "    return np.interp(x =x, xp = xp, fp = fp, left = np.nan, right = np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_COL = [0,1,2,3,4]\n",
    "list_na_representations = ['not_present', 'failed_to_compute']\n",
    "\n",
    "df_pcws_plateaus = pd.read_csv(PCWS_DATA_PLATEAUS, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "df_pcws_transitions = pd.read_csv(PCWS_DATA_TRANSITIONS, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "\n",
    "df_pcws_stats = pd.read_csv(PCWS_DATA_STATS, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "\n",
    "df_pcws_dlogod_timepoints = pd.read_csv(PCWS_DATA_DLOGOD_TIMEPOINTS, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "df_pcws_dlogod_values = pd.read_csv(PCWS_DATA_DLOGOD_VALUES, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "\n",
    "df_pcws_logod_timepoints = pd.read_csv(PCWS_DATA_LOGOD_TIMEPOINTS, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "df_pcws_logod_values = pd.read_csv(PCWS_DATA_LOGOD_VALUES, header = 0, index_col= INDEX_COL,\\\n",
    "                                      float_precision=None, na_values=list_na_representations)\n",
    "\n",
    "\n",
    "\n",
    "def get_piecewise_logod_timeseries(name):\n",
    "    t_array = df_pcws_logod_timepoints.loc[name].dropna()\n",
    "    f_array = df_pcws_logod_values.loc[name].dropna()\n",
    "    return t_array, f_array\n",
    "\n",
    "def get_piecewise_deriv_timsseries(name):\n",
    "    t_array = df_pcws_dlogod_timepoints.loc[name].dropna()\n",
    "    df_array = df_pcws_dlogod_values.loc[name].dropna()\n",
    "    return t_array, df_array\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate number of replicates for each genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_genes = df_pcws_stats.reset_index()['genotype']\n",
    "\n",
    "gene2n = dict()\n",
    "\n",
    "for v in list_genes:\n",
    "    replicates = df_pcws_stats.loc[v]\n",
    "    gene2n[v] = replicates.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(gene2n.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2gene = dict()\n",
    "\n",
    "for k, v in gene2n.items():\n",
    "    n2gene[v] = n2gene.get(v,[]) + [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in n2gene.items():\n",
    "    print(f\"number of genotypes with {k} replicates: {len(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### assign wild-type label\n",
    "def is_wildtype(name):\n",
    "    genotype = name[0]\n",
    "    \n",
    "    if genotype == 'BY4741':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "df_pcws_stats['is_wildtype'] = [is_wildtype(v) for v in df_pcws_stats.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pick subset of curves\n",
    "\n",
    "# onlu include curves with 2 plateaus\n",
    "\n",
    "is_two_plateaus = df_pcws_stats['no_plateaus'] == 2\n",
    "df_subset = df_pcws_stats.loc[is_two_plateaus].copy(deep=True)\n",
    "print(f\"Only include curves with 2 plateaus. Number of curves left: {df_subset.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minimum growth rate to be considered real growth plateau (from histogram)\n",
    "MIN_RATE_FOR_GROWTH = 0.0011\n",
    "fig, ax = plt.subplots(figsize = (FIGWIDTH_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "ax = df_pcws_plateaus.loc[df_subset.index]['mean_value'].hist(bins = 41, color = DATASET_COLOR, log = True, alpha = 0.7,ax =ax)\n",
    "ax.axvline(MIN_RATE_FOR_GROWTH, color= 'red', label = 'min rate for growth')\n",
    "ax.legend(loc = 'upper right')\n",
    "ax.set_xlabel('mean growth rate in plateau')\n",
    "ax.set_ylabel('#plateaus')\n",
    "\n",
    "ax.set_title(f\"n = {df_subset.shape[0]} curves\", loc = 'right') \n",
    "fig.savefig(FIG_DIR + f\"histogram_for_mean_growth_rate_across_plateaus.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plateau should be growth, second plateau should be no growth\n",
    "def name2flag_as_M3(name):\n",
    "    ### get plateaus\n",
    "    plateaus_curve = df_pcws_plateaus.loc[name]\n",
    "    plateau_one, plateau_two = plateaus_curve.iloc[0], plateaus_curve.iloc[1]\n",
    "\n",
    "    ### check conditions\n",
    "    plateau_one_is_growth = np.array(plateau_one['mean_value'] >= MIN_RATE_FOR_GROWTH)\n",
    "    plateau_two_is_stationary = np.array(plateau_two['mean_value'] < MIN_RATE_FOR_GROWTH)\n",
    "\n",
    "    return plateau_one_is_growth & plateau_two_is_stationary\n",
    "\n",
    "## test\n",
    "name = df_subset.index[0]\n",
    "name2flag_as_M3(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_M3_shape = [ name2flag_as_M3(v) for v in df_subset.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_subset.loc[is_M3_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first plateau should be a max type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCENTRATION_GLUCOSE = 20/180 * 1e3 # concentration in milliMolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CONCENTRATION_GLUCOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minimum growth rate to be considered real growth plateau (from histogram)\n",
    "MIN_INITIAL_OD = 0.001\n",
    "fig, ax = plt.subplots(figsize = (FIGWIDTH_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "bins = np.arange(-0.01,0.5,step=0.01)\n",
    "ax = df_raw.loc[df_subset.index]['0'].hist(bins = bins, color = DATASET_COLOR, log = True, alpha = 0.7,ax =ax)\n",
    "ax.axvline(MIN_RATE_FOR_GROWTH, color= 'red', label = 'min rate for growth')\n",
    "ax.legend(loc = 'upper right')\n",
    "ax.set_xlabel('OD value after background substraction')\n",
    "ax.set_ylabel('#plateaus')\n",
    "ax.set_xlim(-0.01,0.01)\n",
    "ax.set_title(f\"n = {df_subset.shape[0]} curves\", loc = 'right') \n",
    "fig.savefig(FIG_DIR + f\"histogram_for_initial_OD.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify point of maximum rate in the instantaneous growath rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def name2yield(name):\n",
    "    ## get timeseries with all timepoints\n",
    "    t, od_excess = get_excess_od_timeseries_before_trim(name)\n",
    "    # estimate OD from timepoint of saturation\n",
    "    plateau_two = df_pcws_plateaus.loc[name].iloc[1]\n",
    "    tsat = (plateau_two['t_end'] + plateau_two['t_start'])/2\n",
    "    od_end = np.interp(x=tsat, xp = t, fp = od_excess)\n",
    "    \n",
    "    # estimate OD from average of initial points\n",
    "    ## find timepoints that are positive\n",
    "    #is_positive = od_excess >0\n",
    "    #log_od_start= np.log(od_excess[is_positive])[:3].mean()\n",
    "    \n",
    "    log_od_start = np.log(od_excess[:3]).mean()\n",
    "    od_start = np.exp(log_od_start)\n",
    "    \n",
    "    ## calculate yield\n",
    "    Y = (od_end - od_start)/CONCENTRATION_GLUCOSE # OD per milliMOlar\n",
    "\n",
    "    return od_start, od_end, Y\n",
    "\n",
    "### test\n",
    "name = df_subset.index[0]\n",
    "od_start,od_end, Y = name2yield(name)\n",
    "\n",
    "def name2max_growth_moment(name):\n",
    "    ### get maximum growth rate\n",
    "    plateau_one = df_pcws_plateaus.loc[name].iloc[0]\n",
    "    gmax = plateau_one['mean_value']\n",
    "    t_gmax = (plateau_one['t_end'] + plateau_one['t_start'])/2\n",
    "    #t_gmax = plateau_one['t_start']\n",
    "    #t_gmax = plateau_one['t_crit']\n",
    "    return t_gmax, gmax\n",
    "## test\n",
    "t_gmax, gmax = name2max_growth_moment(name)\n",
    "\n",
    "\n",
    "def name2lag_time(name):\n",
    "    t, od = get_excess_od_timeseries_before_trim(name)\n",
    "    ## get OD at initial point\n",
    "    od_start, _, _ = name2yield(name)\n",
    "    log_od_start = np.log(od_start)\n",
    "    \n",
    "    # get OD at point of maximum growth moment\n",
    "    t_gmax, gmax = name2max_growth_moment(name)\n",
    "    log_od_gmax = np.interp(x=t_gmax, xp = t, fp = np.log(od)) # we use log to interpolate, since then linear\n",
    "\n",
    "    # use this to infer the lag time\n",
    "    lag_time = t_gmax  + (log_od_start - log_od_gmax)/gmax\n",
    "    \n",
    "    return lag_time\n",
    "\n",
    "## test\n",
    "lag_time = name2lag_time(name)\n",
    "lag_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the M3 like fit to the growth curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for name in df_subset.index:\n",
    "    _, gmax = name2max_growth_moment(name)\n",
    "    df_subset.at[name, 'gmax'] = gmax\n",
    "    lag_time = name2lag_time(name)\n",
    "    df_subset.at[name, 'lag']  = lag_time\n",
    "    od_start,_, Y = name2yield(name)\n",
    "    df_subset.at[name, 'yield'] = Y\n",
    "    df_subset.at[name, 'od_start'] = od_start\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude curves where initial OD is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for nans\n",
    "\n",
    "is_nan = df_subset['lag'].isna() | df_subset['gmax'].isna() | df_subset['yield'].isna()\n",
    "\n",
    "print(is_nan.sum())\n",
    "\n",
    "### exclude\n",
    "\n",
    "df_subset = df_subset.loc[~is_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude curves with negative lag time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_negative_lag = np.array([v < 0 for v in df_subset['lag']])\n",
    "sum(is_negative_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_negative_lag = df_subset.loc[is_negative_lag].sort_values('lag', ascending = True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### exclude\n",
    "\n",
    "df_subset = df_subset.loc[~is_negative_lag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update wild-type index\n",
    "is_wildtype = df_subset['is_wildtype']==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate coverage\n",
    "\n",
    "df_subset.shape[0]/10200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (2*FIGHEIGHT_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "\n",
    "n_datapoints = df_subset.shape[0]\n",
    "is_wildtype = df_subset['is_wildtype']==True\n",
    "\n",
    "ax = axes[0]\n",
    "### plot mutant dataset\n",
    "x= df_subset.loc[~is_wildtype]['gmax'].values\n",
    "y = df_subset.loc[~is_wildtype]['lag'].values\n",
    "ax.scatter(x,y, color = 'silver', label = 'knockout', alpha = 0.6, rasterized = True)\n",
    "## plot wild-type\n",
    "x = df_subset.loc[is_wildtype]['gmax'].values\n",
    "y = df_subset.loc[is_wildtype]['lag'].values\n",
    "ax.scatter(x,y, color = 'tab:green', alpha = 1,  label = 'wild-type', rasterized = True)\n",
    "ax.set_ylabel('lag time [min]')\n",
    "ax.set_xlabel('realized growth rate [per min]')\n",
    "ax.legend(loc =  'upper left')\n",
    "title = f\"n={n_datapoints} growth curves\"\n",
    "ax.set_title(title, loc = 'right')\n",
    "\n",
    "ax = axes[1]\n",
    "### plot mutant dataset\n",
    "x= df_subset.loc[~is_wildtype]['gmax'].values\n",
    "y = df_subset.loc[~is_wildtype]['yield'].values\n",
    "ax.scatter(x,y, color = 'silver', label = 'knockout', alpha = 0.6, rasterized = True)\n",
    "### plot wild-tpe\n",
    "x = df_subset.loc[is_wildtype]['gmax'].values\n",
    "y = df_subset.loc[is_wildtype]['yield'].values\n",
    "ax.scatter(x,y, color = 'tab:green', alpha = 1, label = 'wild-type', rasterized = True)\n",
    "\n",
    "ax.set_ylabel('biomass yield [OD/$\\mu$M glucose]')\n",
    "ax.set_xlabel('realized growth rate [per min]')\n",
    "title = f\"n={n_datapoints} growth curves\"\n",
    "ax.set_title(title, loc = 'right')\n",
    "ax.legend(loc =  'upper left')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(FIG_DIR + f\"correlations_using_M3_traits.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selection_coefficient import Problem_M3, sol_exact_M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def name2Problem_M3(name):\n",
    "    curve = df_subset.loc[name]\n",
    "\n",
    "    strain_params = {'lam':[curve['lag'], 0.], 'g':[curve['gmax'], 1.], 'Y':[curve['yield'],1.]}\n",
    "    initial_conditions = {'R_0':CONCENTRATION_GLUCOSE, 'N_0' : curve['od_start'], 'x': 0.0 }\n",
    "    problem = Problem_M3(**strain_params, **initial_conditions)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "##\n",
    "name = df_subset.index[0]\n",
    "problem = name2Problem_M3(name)\n",
    "problem.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot growth curve\n",
    "\n",
    "\n",
    "def name2plot(name, ax = None):\n",
    "    if ax == None:\n",
    "        fig, ax = plt.subplots(figsize = (FIGWIDTH_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "        \n",
    "\n",
    "    t_full, od_excess = get_excess_od_timeseries_before_trim(name)\n",
    "    ax.scatter(t_full, od_excess, marker = 'o', color = 'navy', label = 'before trim')\n",
    "    \n",
    "    t_trimmed, od_trimmed = get_excess_od_timeseries(name)\n",
    "    ax.scatter(t_trimmed, od_trimmed, marker = 'o', color = 'tab:orange', label = 'after trim')\n",
    "\n",
    "    #t, logod_pcws = get_piecewise_logod_timeseries(name)\n",
    "    #ax.plot(t, np.exp(logod_pcws), color = 'tab:blue', label = 'pcws fit', ls= '--')\n",
    "\n",
    "    problem = name2Problem_M3(name)\n",
    "    fit = [sol_exact_M3(t=v, problem=problem) for v in t_full]\n",
    "    ax.plot(t_full,fit, color = 'black', label = 'M3 fit')\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('time t')\n",
    "    ax.set_ylabel('populationn size [OD]')\n",
    "    ax.legend(loc = 'lower right')\n",
    "    \n",
    "    ax.set_ylim(ymin = 0.0005, ymax = 10)\n",
    "    title = \"curve id: \" + str(name)\n",
    "    ax.set_title(title, loc = 'right')\n",
    "    ax.set_xlim(0,3000)\n",
    "    return ax\n",
    "\n",
    "## test\n",
    "\n",
    "name = df_subset.index[is_wildtype][0]\n",
    "ax = name2plot(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (FIGWIDTH_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "\n",
    "t, growth_rate = get_piecewise_deriv_timsseries(name)\n",
    "\n",
    "ax.plot(t,growth_rate)\n",
    "ax.set_ylabel('growth rate')\n",
    "ax.set_xlim(xmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate rsquared to growth curve\n",
    "\n",
    "\n",
    "\n",
    "def name2rsquared(name):\n",
    "    t, od_excess = get_excess_od_timeseries_before_trim(name)\n",
    "    problem = name2Problem_M3(name)\n",
    "    fit = [sol_exact_M3(t=v, problem=problem) for v in t]\n",
    "\n",
    "    ## calculaate rsquared on a log-scale\n",
    "    y_hat = np.log(od_excess)\n",
    "    y = np.log(fit)\n",
    "    sum_residuals = np.power(y_hat -y,2).sum()\n",
    "    sum_total = np.power(y_hat - y_hat.mean(),2).sum()\n",
    "\n",
    "    rsquared = 1 - (sum_residuals/sum_total)\n",
    "    return rsquared\n",
    "\n",
    "## test\n",
    "name = df_subset.index[0]\n",
    "name2rsquared(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in df_subset.index:\n",
    "    df_subset.at[name, 'rsquared'] = name2rsquared(name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_RSQUARED_TO_INCLUDE = 0.95\n",
    "fig, ax = plt.subplots(figsize = (FIGWIDTH_TRIPLET, FIGHEIGHT_TRIPLET))\n",
    "ax = df_subset['rsquared'].hist(bins = 41, color = DATASET_COLOR, log = True, alpha = 0.7, ax =ax)\n",
    "ax.axvline(MIN_RSQUARED_TO_INCLUDE, color = 'tab:red', label = 'minimum quality of fit required')\n",
    "ax.set_xlabel('quality of fit: $R^2$')\n",
    "ax.set_ylabel('#curves')\n",
    "ax.legend()\n",
    "ax.set_title(f\"n = {df_subset.shape[0]} curves\", loc = 'right') \n",
    "\n",
    "fig.savefig(FIG_DIR + f\"histogram_for_rsquared_after_fit.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_low_quality = np.array([v < MIN_RSQUARED_TO_INCLUDE for v in df_subset['rsquared']])\n",
    "sum(is_low_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_low_quality = df_subset.loc[is_low_quality].sort_values('rsquared', ascending = True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,3, figsize = (3*FIGWIDTH_TRIPLET, 4*FIGHEIGHT_TRIPLET))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "\n",
    "for ax,i in zip(axes, range(12)):\n",
    "    \n",
    "    name = list_low_quality[i]\n",
    "    name2plot(name, ax = ax)\n",
    "    rsquared = df_subset.at[name,'rsquared']\n",
    "    title = f\"$R^2 = {rsquared:.2f}$, {name}\"\n",
    "    ax.set_title(title, loc = 'right')\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(FIG_DIR + f\"growthcurves_for_outliers_with_low_rsquared.pdf\", DPI = DPI, bbox_inches = 'tight', pad_inches = PAD_INCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### exclude low quality fits\n",
    "\n",
    "df_subset = df_subset.loc[~is_low_quality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop columns\n",
    "list_cols_to_drop = ['plat_threshold', 'plat_duration', 'plat_distance', 'plat_atol', 'plat_rtol',\\\n",
    "                     'tran_threshold', 'tran_duration', 'tran_distance', 'tran_atol','tran_rtol',\\\n",
    "                    'no_plateaus', 'no_mono_violations',  'curve_rsquared_fd', 'final_gap_logod']\n",
    "\n",
    "df_subset= df_subset.drop(list_cols_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### store dataset\n",
    "PCWS_OUTPUT_TRAITS = OUTPUT_DIR + \"df_M3_traits.csv\"\n",
    "df_subset.to_csv(PCWS_OUTPUT_TRAITS, index = True, float_format= '%.6e', na_rep= 'removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reread and test\n",
    "\n",
    "df = df_subset\n",
    "filename = PCWS_OUTPUT_TRAITS\n",
    "\n",
    "print('#####################################')\n",
    "print('\\nTesting the data stored in ' + filename)\n",
    "df_reread = pd.read_csv(filename, header = 0, index_col= INDEX_COL,\\\n",
    "                                  float_precision=None, na_values=list_na_representations)\n",
    "print(\"Testing stored float values.\")\n",
    "float_columns = df.dtypes == 'float64'\n",
    "\n",
    "x = df_reread.loc[:,float_columns].values\n",
    "y = df.loc[:,float_columns].values\n",
    "\n",
    "try:\n",
    "    np.testing.assert_array_equal(x,y)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"\\nTesting stored values of other type, mostly strings.\")\n",
    "other_columns = ~float_columns\n",
    "x = df_reread.loc[:,other_columns]\n",
    "y = df.loc[:,other_columns]\n",
    "\n",
    "\n",
    "try:\n",
    "    assert x.equals(y)\n",
    "    print(\"Success. All values of other type stored correctly.\")\n",
    "except Exception as e:\n",
    "    print(\"Fail. Check true datatypes for columns marked as other in dataframe.\")\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual outlier inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gmax_high = df_subset.sort_values('gmax', ascending = False).index[:10]\n",
    "list_gmax_low = df_subset.sort_values('gmax', ascending = True).index[:10]\n",
    "list_lag_high = df_subset.sort_values('lag', ascending = False).index[:10]\n",
    "list_lag_low = df_subset.sort_values('lag', ascending = True).index[:10]\n",
    "list_yield_high = df_subset.sort_values('yield', ascending = False).index[:10]\n",
    "list_yield_low = df_subset.sort_values('yield', ascending = True).index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3, figsize = (3*FIGWIDTH_TRIPLET, 2*FIGHEIGHT_TRIPLET))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "for ax,i in zip(axes, range(6)):\n",
    "\n",
    "    name2plot(list_yield_low[i], ax = ax)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
